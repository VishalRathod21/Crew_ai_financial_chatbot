2025-09-02 22:42:23,844 - __main__ - INFO - Starting Financial Market Crew...
2025-09-02 22:42:23,845 - __main__ - ERROR - Error in Financial Market Crew execution: 1 validation error for Crew
verbose
  Input should be a valid boolean, unable to interpret input [type=bool_parsing, input_value=2, input_type=int]
    For further information visit https://errors.pydantic.dev/2.11/v/bool_parsing
Traceback (most recent call last):
  File "/home/vishalr/Documents/CrewAIAgent/financial_market_crew.py", line 188, in run
    crew = Crew(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for Crew
verbose
  Input should be a valid boolean, unable to interpret input [type=bool_parsing, input_value=2, input_type=int]
    For further information visit https://errors.pydantic.dev/2.11/v/bool_parsing
2025-09-02 22:42:23,847 - __main__ - CRITICAL - Critical error in main execution: 1 validation error for Crew
verbose
  Input should be a valid boolean, unable to interpret input [type=bool_parsing, input_value=2, input_type=int]
    For further information visit https://errors.pydantic.dev/2.11/v/bool_parsing
Traceback (most recent call last):
  File "/home/vishalr/Documents/CrewAIAgent/financial_market_crew.py", line 229, in <module>
    result = market_crew.run()
  File "/home/vishalr/Documents/CrewAIAgent/financial_market_crew.py", line 188, in run
    crew = Crew(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/pydantic/main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for Crew
verbose
  Input should be a valid boolean, unable to interpret input [type=bool_parsing, input_value=2, input_type=int]
    For further information visit https://errors.pydantic.dev/2.11/v/bool_parsing
2025-09-02 22:59:30,943 - __main__ - INFO - Starting Financial Market Crew...
2025-09-02 22:59:31,076 - __main__ - ERROR - Error in Financial Market Crew execution: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=models/gemini-2.0-flash-exp
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
Traceback (most recent call last):
  File "/home/vishalr/Documents/CrewAIAgent/financial_market_crew.py", line 197, in run
    result = crew.kickoff()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 446, in _execute_core
    result = agent.execute_task(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 475, in execute_task
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/utilities/agent_utils.py", line 160, in get_llm_response
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/utilities/agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/llm.py", line 1030, in call
    return self._handle_non_streaming_response(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/utils.py", line 1330, in wrapper
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/main.py", line 3427, in completion
    raise exception_type(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/main.py", line 1097, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 391, in get_llm_provider
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 368, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=models/gemini-2.0-flash-exp
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-02 22:59:31,083 - __main__ - CRITICAL - Critical error in main execution: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=models/gemini-2.0-flash-exp
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
Traceback (most recent call last):
  File "/home/vishalr/Documents/CrewAIAgent/financial_market_crew.py", line 229, in <module>
    result = market_crew.run()
  File "/home/vishalr/Documents/CrewAIAgent/financial_market_crew.py", line 197, in run
    result = crew.kickoff()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 446, in _execute_core
    result = agent.execute_task(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 475, in execute_task
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/utilities/agent_utils.py", line 160, in get_llm_response
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/utilities/agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/llm.py", line 1030, in call
    return self._handle_non_streaming_response(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/utils.py", line 1330, in wrapper
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/main.py", line 3427, in completion
    raise exception_type(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/main.py", line 1097, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 391, in get_llm_provider
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 368, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=models/gemini-2.0-flash-exp
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-02 23:07:08,291 - __main__ - INFO - Starting Financial Market Crew...
2025-09-02 23:07:08,442 - __main__ - ERROR - Error in Financial Market Crew execution: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model={'model': 'gemini-pro', 'api_key': 'AIzaSyA0N1vJ7YT3tHgpWjfkCMmKmoJM0xzte1Y', 'temperature': 0.1, 'max_tokens': 2000}
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
Traceback (most recent call last):
  File "/home/vishalr/Documents/CrewAIAgent/financial_market_crew.py", line 203, in run
    result = crew.kickoff()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 446, in _execute_core
    result = agent.execute_task(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 475, in execute_task
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/utilities/agent_utils.py", line 160, in get_llm_response
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/utilities/agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/llm.py", line 1030, in call
    return self._handle_non_streaming_response(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/utils.py", line 1330, in wrapper
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/main.py", line 3427, in completion
    raise exception_type(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/main.py", line 1097, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 391, in get_llm_provider
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 368, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model={'model': 'gemini-pro', 'api_key': 'AIzaSyA0N1vJ7YT3tHgpWjfkCMmKmoJM0xzte1Y', 'temperature': 0.1, 'max_tokens': 2000}
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-02 23:07:08,449 - __main__ - CRITICAL - Critical error in main execution: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model={'model': 'gemini-pro', 'api_key': 'AIzaSyA0N1vJ7YT3tHgpWjfkCMmKmoJM0xzte1Y', 'temperature': 0.1, 'max_tokens': 2000}
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
Traceback (most recent call last):
  File "/home/vishalr/Documents/CrewAIAgent/financial_market_crew.py", line 235, in <module>
    result = market_crew.run()
  File "/home/vishalr/Documents/CrewAIAgent/financial_market_crew.py", line 203, in run
    result = crew.kickoff()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/task.py", line 446, in _execute_core
    result = agent.execute_task(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 475, in execute_task
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/agents/crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/utilities/agent_utils.py", line 160, in get_llm_response
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/utilities/agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/llm.py", line 1030, in call
    return self._handle_non_streaming_response(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/crewai/llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/utils.py", line 1330, in wrapper
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/main.py", line 3427, in completion
    raise exception_type(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/main.py", line 1097, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 391, in get_llm_provider
    raise e
  File "/home/vishalr/Documents/CrewAIAgent/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 368, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model={'model': 'gemini-pro', 'api_key': 'AIzaSyA0N1vJ7YT3tHgpWjfkCMmKmoJM0xzte1Y', 'temperature': 0.1, 'max_tokens': 2000}
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
